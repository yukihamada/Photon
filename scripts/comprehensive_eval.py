#!/usr/bin/env python3
"""
Comprehensive evaluation of Japanese LLMs with varied questions
2025-2026 models comparison
"""
import subprocess
import json
import time
import re

# Models to compare (2025-2026 releases + base model)
MODELS = [
    {
        "name": "Photon-1.7B",
        "path": "./outputs/Photon-1.7B-Instruct-v1-Q5_K_M.gguf",
        "size": "1.2GB",
        "params": "1.7B",
        "developer": "yukihamada",
        "release": "2025/01",
        "chat_format": "chatml",
    },
    {
        "name": "Qwen3-1.7B",
        "path": "./outputs/comparison_models/Qwen3-1.7B-Q8_0.gguf",
        "size": "1.7GB",
        "params": "1.7B",
        "developer": "Alibaba",
        "release": "2025/04",
        "chat_format": "chatml",
    },
    {
        "name": "TinySwallow-1.5B",
        "path": "./outputs/comparison_models/TinySwallow-1.5B-Instruct-Q5_K_M.gguf",
        "size": "1.0GB",
        "params": "1.5B",
        "developer": "Sakana AI",
        "release": "2025",
        "chat_format": "chatml",
    },
    {
        "name": "Sarashina2.2-3B",
        "path": "./outputs/comparison_models/Sarashina2.2-3B-Q4_K_M.gguf",
        "size": "1.9GB",
        "params": "3B",
        "developer": "SB Intuitions",
        "release": "2024/12",
        "chat_format": "chatml",
    },
]

# Varied test questions
TEST_QUESTIONS = [
    # æ•°å­¦ãƒ»è¨ˆç®—
    {"id": "math_1", "category": "æ•°å­¦", "q": "2ã®10ä¹—ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚", "answer_contains": ["1024"]},
    {"id": "math_2", "category": "æ•°å­¦", "q": "1+2+3+...+10ã®åˆè¨ˆã¯ï¼Ÿ", "answer_contains": ["55"]},
    {"id": "math_3", "category": "æ•°å­¦", "q": "100ã‚’7ã§å‰²ã£ãŸä½™ã‚Šã¯ï¼Ÿ", "answer_contains": ["2"]},

    # è«–ç†ãƒ»æ¨è«–
    {"id": "logic_1", "category": "è«–ç†", "q": "Aã¯Bã‚ˆã‚ŠèƒŒãŒé«˜ã„ã€‚Bã¯Cã‚ˆã‚ŠèƒŒãŒé«˜ã„ã€‚ä¸€ç•ªèƒŒãŒé«˜ã„ã®ã¯èª°ï¼Ÿ", "answer_contains": ["A"]},
    {"id": "logic_2", "category": "è«–ç†", "q": "å…¨ã¦ã®çŠ¬ã¯å‹•ç‰©ã§ã‚ã‚‹ã€‚ãƒãƒã¯çŠ¬ã§ã‚ã‚‹ã€‚ãƒãƒã¯å‹•ç‰©ã‹ï¼Ÿ", "answer_contains": ["ã¯ã„", "å‹•ç‰©", "Yes"]},
    {"id": "logic_3", "category": "è«–ç†", "q": "ã‚Šã‚“ã”ãŒ3å€‹ã€ã¿ã‹ã‚“ãŒ5å€‹ã‚ã‚Šã¾ã™ã€‚æœç‰©ã¯å…¨éƒ¨ã§ä½•å€‹ï¼Ÿ", "answer_contains": ["8"]},

    # æ—¥æœ¬èªãƒ»æ–‡åŒ–
    {"id": "jp_1", "category": "æ—¥æœ¬èª", "q": "ã€Œä¸€æœŸä¸€ä¼šã€ã®æ„å‘³ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚", "answer_contains": ["ä¸€åº¦", "å‡ºä¼šã„", "å¤§åˆ‡"]},
    {"id": "jp_2", "category": "æ—¥æœ¬èª", "q": "ã€ŒçŒ«ã«å°åˆ¤ã€ã¯ã©ã†ã„ã†æ„å‘³ï¼Ÿ", "answer_contains": ["ä¾¡å€¤", "ã‚ã‹ã‚‰ãªã„", "ç„¡é§„"]},
    {"id": "jp_3", "category": "æ—¥æœ¬èª", "q": "ã€Œã•ãã‚‰ã€ã‚’ä½¿ã£ãŸçŸ­ã„æ–‡ã‚’ä½œã£ã¦ãã ã•ã„ã€‚", "answer_contains": []},

    # çŸ¥è­˜ãƒ»å¸¸è­˜
    {"id": "know_1", "category": "çŸ¥è­˜", "q": "æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ", "answer_contains": ["æ±äº¬"]},
    {"id": "know_2", "category": "çŸ¥è­˜", "q": "1å¹´ã¯ä½•æ—¥ã§ã™ã‹ï¼Ÿ", "answer_contains": ["365", "366"]},
    {"id": "know_3", "category": "çŸ¥è­˜", "q": "æ°´ã®åŒ–å­¦å¼ã¯ï¼Ÿ", "answer_contains": ["H2O"]},

    # å‰µä½œ
    {"id": "creative_1", "category": "å‰µä½œ", "q": "æ˜¥ã‚’ãƒ†ãƒ¼ãƒã«ä¿³å¥ã‚’ä¸€ã¤è© ã‚“ã§ãã ã•ã„ã€‚", "answer_contains": []},
    {"id": "creative_2", "category": "å‰µä½œ", "q": "ã€Œå¸Œæœ›ã€ã‚’ãƒ†ãƒ¼ãƒã«ä¸€è¡Œè©©ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚", "answer_contains": []},

    # èª¬æ˜ãƒ»è§£èª¬
    {"id": "explain_1", "category": "èª¬æ˜", "q": "ãªãœç©ºã¯é’ã„ã®ã§ã™ã‹ï¼Ÿç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚", "answer_contains": ["å…‰", "æ•£ä¹±"]},
    {"id": "explain_2", "category": "èª¬æ˜", "q": "AIã¨ã¯ä½•ã§ã™ã‹ï¼Ÿä¸€æ–‡ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚", "answer_contains": ["äººå·¥", "çŸ¥èƒ½"]},

    # å®Ÿç”¨
    {"id": "practical_1", "category": "å®Ÿç”¨", "q": "é¢¨é‚ªã‚’å¼•ã„ãŸæ™‚ã®å¯¾å‡¦æ³•ã‚’3ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚", "answer_contains": []},
    {"id": "practical_2", "category": "å®Ÿç”¨", "q": "ãŠã™ã™ã‚ã®æœé£Ÿãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚", "answer_contains": []},

    # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹
    {"id": "edge_1", "category": "å¢ƒç•Œ", "q": "ç§ã®èª•ç”Ÿæ—¥ã¯ã„ã¤ã§ã™ã‹ï¼Ÿ", "answer_contains": ["ã‚ã‹ã‚Šã¾ã›ã‚“", "çŸ¥ã‚Šã¾ã›ã‚“", "æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“", "å­˜ã˜ã¾ã›ã‚“"]},
    {"id": "edge_2", "category": "å¢ƒç•Œ", "q": "æ˜æ—¥ã®å¤©æ°—ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚", "answer_contains": ["ã‚ã‹ã‚Šã¾ã›ã‚“", "äºˆæ¸¬ã§ãã¾ã›ã‚“", "æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“", "ç¢ºèª"]},
]


def run_inference(model_path: str, prompt: str, chat_format: str = "chatml", timeout: int = 90) -> tuple:
    """Run inference and return response with timing"""
    if chat_format == "chatml":
        full_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    else:
        full_prompt = f"User: {prompt}\nAssistant:"

    cmd = [
        "llama-cli",
        "-m", model_path,
        "-p", full_prompt,
        "-n", "200",
        "--temp", "0.7",
        "--no-display-prompt",
        "-c", "2048",
    ]

    start = time.time()
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)
        elapsed = time.time() - start
        response = result.stdout.strip().replace("[end of text]", "").strip()
        return response, elapsed
    except subprocess.TimeoutExpired:
        return "[TIMEOUT]", timeout
    except Exception as e:
        return f"[ERROR: {e}]", 0


def check_answer(response: str, expected: list) -> bool:
    """Check if response contains expected keywords"""
    if not expected:
        return len(response) > 10  # Just check it generated something
    return any(kw.lower() in response.lower() for kw in expected)


def has_thinking_tags(response: str) -> bool:
    """Check for <think> tags"""
    return "<think>" in response and "</think>" in response


def main():
    print("=" * 100)
    print("æ—¥æœ¬èªLLM ç·åˆè©•ä¾¡ - 2025-2026å¹´ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ")
    print("=" * 100)

    print("\n## è©•ä¾¡å¯¾è±¡ãƒ¢ãƒ‡ãƒ«\n")
    print(f"| {'ãƒ¢ãƒ‡ãƒ«':<20} | {'ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿':<10} | {'ã‚µã‚¤ã‚º':<8} | {'é–‹ç™ºå…ƒ':<15} | {'ãƒªãƒªãƒ¼ã‚¹':<10} |")
    print("|" + "-"*22 + "|" + "-"*12 + "|" + "-"*10 + "|" + "-"*17 + "|" + "-"*12 + "|")
    for m in MODELS:
        print(f"| {m['name']:<20} | {m['params']:<10} | {m['size']:<8} | {m['developer']:<15} | {m['release']:<10} |")

    results = {m["name"]: {"correct": 0, "thinking": 0, "total": 0, "time": 0, "responses": []} for m in MODELS}

    for q in TEST_QUESTIONS:
        print(f"\n{'='*100}")
        print(f"## [{q['id']}] {q['category']}: {q['q']}")
        print("=" * 100)

        for model in MODELS:
            response, elapsed = run_inference(model["path"], q["q"], model["chat_format"])

            correct = check_answer(response, q["answer_contains"])
            thinking = has_thinking_tags(response)

            results[model["name"]]["total"] += 1
            results[model["name"]]["time"] += elapsed
            if correct:
                results[model["name"]]["correct"] += 1
            if thinking:
                results[model["name"]]["thinking"] += 1

            results[model["name"]]["responses"].append({
                "id": q["id"],
                "response": response[:500],
                "correct": correct,
                "thinking": thinking,
                "time": elapsed,
            })

            # Display
            status = "âœ…" if correct else "âŒ"
            think_icon = "ğŸ§ " if thinking else "  "
            print(f"\n### {model['name']} {status} {think_icon} ({elapsed:.1f}s)")
            print("-" * 60)

            # Show response (truncated)
            display_response = response[:300].replace("\n", "\n> ")
            print(f"> {display_response}")
            if len(response) > 300:
                print("> ...")

    # Summary
    print("\n" + "=" * 100)
    print("## ç·åˆè©•ä¾¡ã‚µãƒãƒªãƒ¼")
    print("=" * 100)

    print(f"\n| {'ãƒ¢ãƒ‡ãƒ«':<20} | {'æ­£ç­”ç‡':<10} | {'æ€è€ƒã‚¿ã‚°':<10} | {'å¹³å‡æ™‚é–“':<10} | {'ç·åˆ':<10} |")
    print("|" + "-"*22 + "|" + "-"*12 + "|" + "-"*12 + "|" + "-"*12 + "|" + "-"*12 + "|")

    for model in MODELS:
        r = results[model["name"]]
        acc = r["correct"] / r["total"] * 100
        think = r["thinking"] / r["total"] * 100
        avg_time = r["time"] / r["total"]
        score = (acc + think) / 2

        print(f"| {model['name']:<20} | {acc:>6.1f}%   | {think:>6.1f}%   | {avg_time:>6.1f}s   | {score:>6.1f}   |")

    # Category breakdown
    print("\n## ã‚«ãƒ†ã‚´ãƒªåˆ¥æ­£ç­”ç‡ (Photon-1.7B)")
    print("-" * 60)

    categories = {}
    for q in TEST_QUESTIONS:
        cat = q["category"]
        if cat not in categories:
            categories[cat] = {"total": 0, "correct": 0}
        categories[cat]["total"] += 1

    for resp in results["Photon-1.7B"]["responses"]:
        q_data = next(q for q in TEST_QUESTIONS if q["id"] == resp["id"])
        if resp["correct"]:
            categories[q_data["category"]]["correct"] += 1

    for cat, stats in categories.items():
        rate = stats["correct"] / stats["total"] * 100
        bar = "â–ˆ" * int(rate / 10) + "â–‘" * (10 - int(rate / 10))
        print(f"  {cat:<10}: {bar} {stats['correct']}/{stats['total']} ({rate:.0f}%)")

    # Save results
    with open("./outputs/comprehensive_eval_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print("\n" + "=" * 100)
    print("çµæœã‚’ ./outputs/comprehensive_eval_results.json ã«ä¿å­˜ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()
