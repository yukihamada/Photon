#!/usr/bin/env python3
"""
Compare responses from different Japanese LLMs with actual output
"""
import subprocess
import json
import time

# Models to compare
MODELS = [
    {
        "name": "Photon-1.7B",
        "path": "./outputs/Photon-1.7B-Instruct-v1-Q5_K_M.gguf",
        "size": "1.2GB",
        "params": "1.7B",
        "chat_format": "chatml",
    },
    {
        "name": "TinySwallow-1.5B",
        "path": "./outputs/comparison_models/TinySwallow-1.5B-Instruct-Q5_K_M.gguf",
        "size": "1.0GB",
        "params": "1.5B",
        "chat_format": "chatml",
    },
    {
        "name": "RakutenAI-7B",
        "path": "./outputs/comparison_models/RakutenAI-7B-instruct-q4_K_M.gguf",
        "size": "4.2GB",
        "params": "7B",
        "chat_format": "mistral",  # Rakuten uses Mistral format
    },
]

# Test prompts
TEST_PROMPTS = [
    {
        "id": "math",
        "prompt": "2ã®10ä¹—ã¯ã„ãã¤ã§ã™ã‹ï¼Ÿè¨ˆç®—éç¨‹ã‚‚æ•™ãˆã¦ãã ã•ã„ã€‚",
        "category": "æ•°å­¦",
    },
    {
        "id": "logic",
        "prompt": "Aã¯Bã‚ˆã‚ŠèƒŒãŒé«˜ã„ã€‚Bã¯Cã‚ˆã‚ŠèƒŒãŒé«˜ã„ã€‚ä¸€ç•ªèƒŒãŒé«˜ã„ã®ã¯èª°ï¼Ÿ",
        "category": "è«–ç†",
    },
    {
        "id": "culture",
        "prompt": "ã€Œä¸€æœŸä¸€ä¼šã€ã®æ„å‘³ã‚’ç°¡æ½”ã«æ•™ãˆã¦ãã ã•ã„ã€‚",
        "category": "æ—¥æœ¬æ–‡åŒ–",
    },
    {
        "id": "creative",
        "prompt": "æ˜¥ã‚’ãƒ†ãƒ¼ãƒã«ä¿³å¥ã‚’ä¸€ã¤è© ã‚“ã§ãã ã•ã„ã€‚",
        "category": "å‰µä½œ",
    },
]


def run_inference(model_path: str, prompt: str, chat_format: str, timeout: int = 90) -> tuple:
    """Run inference and return response with timing"""

    # Format prompt based on model
    if chat_format == "chatml":
        full_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    elif chat_format == "mistral":
        full_prompt = f"[INST] {prompt} [/INST]"
    else:
        full_prompt = f"User: {prompt}\nAssistant:"

    cmd = [
        "llama-cli",
        "-m", model_path,
        "-p", full_prompt,
        "-n", "300",
        "--temp", "0.7",
        "--no-display-prompt",
        "-c", "2048",
    ]

    start = time.time()
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=timeout,
        )
        elapsed = time.time() - start
        response = result.stdout.strip()
        # Clean up response
        response = response.replace("[end of text]", "").strip()
        return response, elapsed
    except subprocess.TimeoutExpired:
        return "[TIMEOUT]", timeout
    except Exception as e:
        return f"[ERROR: {e}]", 0


def main():
    print("=" * 80)
    print("æ—¥æœ¬èªLLM æ¯”è¼ƒè©•ä¾¡ - å®Ÿéš›ã®å¿œç­”ã‚’ç¢ºèª")
    print("=" * 80)

    print("\n## è©•ä¾¡å¯¾è±¡ãƒ¢ãƒ‡ãƒ«\n")
    print("| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ã‚µã‚¤ã‚º | é–‹ç™ºå…ƒ |")
    print("|--------|-----------|--------|--------|")
    for m in MODELS:
        developer = "yukihamada" if "Photon" in m["name"] else "Sakana AI" if "Swallow" in m["name"] else "Rakuten"
        print(f"| {m['name']} | {m['params']} | {m['size']} | {developer} |")

    results = {}

    for test in TEST_PROMPTS:
        print(f"\n{'=' * 80}")
        print(f"## ãƒ†ã‚¹ãƒˆ: {test['category']} - {test['id']}")
        print(f"**è³ªå•**: {test['prompt']}")
        print("=" * 80)

        results[test["id"]] = {}

        for model in MODELS:
            print(f"\n### {model['name']} ({model['params']})")
            print("-" * 40)

            response, elapsed = run_inference(
                model["path"],
                test["prompt"],
                model["chat_format"]
            )

            results[test["id"]][model["name"]] = {
                "response": response,
                "time": elapsed,
            }

            # Check for thinking tags
            has_thinking = "<think>" in response and "</think>" in response
            thinking_status = "ğŸ§  æ€è€ƒã‚ã‚Š" if has_thinking else ""

            print(f"**å¿œç­”æ™‚é–“**: {elapsed:.1f}ç§’ {thinking_status}")
            print(f"\n```")
            # Truncate very long responses
            if len(response) > 500:
                print(response[:500] + "...")
            else:
                print(response)
            print("```")

    # Summary
    print("\n" + "=" * 80)
    print("## ç·åˆè©•ä¾¡ã‚µãƒãƒªãƒ¼")
    print("=" * 80)

    print("\n| ãƒ¢ãƒ‡ãƒ« | å¹³å‡å¿œç­”æ™‚é–“ | æ€è€ƒã‚¿ã‚° | ç‰¹å¾´ |")
    print("|--------|-------------|----------|------|")

    for model in MODELS:
        total_time = 0
        thinking_count = 0
        for test_id, test_results in results.items():
            if model["name"] in test_results:
                total_time += test_results[model["name"]]["time"]
                if "<think>" in test_results[model["name"]]["response"]:
                    thinking_count += 1

        avg_time = total_time / len(TEST_PROMPTS)
        thinking_pct = thinking_count / len(TEST_PROMPTS) * 100

        if "Photon" in model["name"]:
            feature = "æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹å¯è¦–åŒ–"
        elif "Rakuten" in model["name"]:
            feature = "å¤§è¦æ¨¡ãƒ»é«˜ç²¾åº¦"
        else:
            feature = "è»½é‡ãƒ»é«˜é€Ÿ"

        print(f"| {model['name']} | {avg_time:.1f}ç§’ | {thinking_pct:.0f}% | {feature} |")

    # Save results
    with open("./outputs/comparison_responses.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print("\nçµæœã‚’ ./outputs/comparison_responses.json ã«ä¿å­˜ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()
