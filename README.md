# Photon-1.7B (Instruct-v1)

Qwen3-1.7Bベースの日本語思考モデル。iPhoneでローカル動作可能。

## 特徴

- **日本語思考**: `<think>...</think>` タグで推論プロセスを表示
- **ツール呼び出し**: `<tool_call>` 形式でアプリ連携
- **軽量**: 1.7Bパラメータ、Q5_K_M量子化で約1.2GB
- **多彩な口調**: 285種のキャラクター憑依システムプロンプト

## 学習データ (v3)

| カテゴリ | 件数 |
|---------|------|
| コア推論（論理・数学・推論） | 515 |
| 日本語・文化 | 193 |
| 会話・ユーモア | 116 |
| 実用・教養 | 153 |
| ニッチジャンル | 54 |
| 時事・トレンド | 33 |
| **合計** | **1,166** |

## クイックスタート

### Lambda Labsで学習

```bash
git clone https://github.com/yukihamada/Photon.git
cd Photon
pip install unsloth
python scripts/train_unsloth.py
```

### データ形式

```json
{
  "messages": [
    {"role": "system", "content": "あなたはルフィ風。「〜だ！」と元気よく話す。"},
    {"role": "user", "content": "2の10乗は？"},
    {"role": "assistant", "content": "<think>\n2を10回かけるんだな！\n2×2=4、4×2=8...\n</think>\n\n1024だ！海賊王に俺はなる！"}
  ]
}
```

## ディレクトリ構造

```
Photon/
├── data/
│   ├── eliochat_v3_merged.jsonl  # マージ済み学習データ
│   └── *.jsonl                    # 個別カテゴリデータ
├── scripts/
│   ├── train_unsloth.py          # Unsloth学習スクリプト
│   ├── data_generation/          # データ生成スクリプト群
│   └── merge_all_data.py         # データマージ
└── outputs/                       # 学習済みモデル出力先
```

## モデル仕様

| 項目 | 値 |
|------|-----|
| ベースモデル | Qwen/Qwen3-1.7B |
| 学習方式 | LoRA (PEFT) |
| LoRA Rank | 64 |
| コンテキスト長 | 4,096 |
| 量子化 | Q5_K_M (日本語Imatrix) |

## 学習結果 (v1)

| 項目 | 値 |
|------|-----|
| 学習時間 | 32.7分 |
| GPU | NVIDIA A100-SXM4-40GB |
| コスト | $0.70 |
| 最終Loss | 1.25 |
| Epochs | 2 |
| バッチサイズ | 2 (accumulation: 8) |
| 学習率 | 2e-5 |

## ベンチマーク比較

日本語LLMとの比較評価（2025年1月）

### 評価モデル

| モデル | パラメータ | サイズ | 開発元 |
|--------|-----------|--------|--------|
| **Photon-1.7B** | 1.7B | 1.2 GB | yukihamada |
| TinySwallow-1.5B | 1.5B | 1.0 GB | Sakana AI |
| RakutenAI-7B | 7B | 4.2 GB | Rakuten |

### 総合評価

| モデル | 平均応答時間 | 思考タグ | 特徴 |
|--------|-------------|----------|------|
| **Photon-1.7B** | 23.9秒 | **100%** | 思考プロセス可視化 |
| TinySwallow-1.5B | 10.3秒 | 0% | 軽量・高速 |
| RakutenAI-7B | 32.2秒 | 0% | 大規模モデル |

### 実際の応答比較

#### 質問1: 「2の10乗はいくつですか？計算過程も教えてください。」

**Photon-1.7B** 🧠
```
<think>
1. 2の1乗：2^1 = 2
2. 2の2乗：2^2 = 2 × 2 = 4
3. 2の3乗：2^3 = 2 × 2 × 2 = 8
...
10. 2の10乗：2^10 = 1024
</think>
```

**TinySwallow-1.5B**
```
2の10乗は1024です。
2^10 = 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 = 1024
```

**RakutenAI-7B**
```
2の10乗は1,024です。
```

#### 質問2: 「AはBより背が高い。BはCより背が高い。一番背が高いのは誰？」

**Photon-1.7B** 🧠 ✅正解
```
<think>
AはBより背が高い、BはCより背が高いという情報から、
AがCよりも高さをもつことが推論できます。
</think>

A > B > C となる。一番背が高いのは **A** です。
```

**TinySwallow-1.5B** ❌誤答
```
一番背が高いのはCです。（誤り）
A > B > C となります。
```

**RakutenAI-7B** ⚠️ループ発生
```
[INST] 1. AはBより背が高い...（繰り返し）
```

#### 質問3: 「一期一会」の意味

**Photon-1.7B** 🧠
```
<think>
「一期」は「一度」、「一会」は「会う」を意味し...
</think>

人生の中で一度だけの出会いを大切にする精神
```

**TinySwallow-1.5B**
```
一生に一度だけの出会いを大切にすること
禅の教えから来ています。
```

### Photonの強み

| 特徴 | Photon | 他モデル |
|------|--------|----------|
| 思考過程の可視化 | ✅ 常に`<think>`タグで表示 | ❌ なし |
| 論理推論の正確性 | ✅ 推論過程を明示 | △ 結論のみ/誤答あり |
| 透明性 | ◎ なぜその答えか分かる | △ ブラックボックス |
| iPhoneサイズ | ✅ 1.2GB | ✅〜❌ 1.0〜4.2GB |

> **結論**: Photonは「なぜその答えなのか」が見える唯一のモデル。小規模ながら推論の透明性で差別化。

## ダウンロード

### GGUF (iPhone/ローカル用)

| ファイル | サイズ | 用途 |
|----------|--------|------|
| `Photon-1.7B-Instruct-v1-Q5_K_M.gguf` | 1.2 GB | 推奨 (バランス型) |
| `Photon-1.7B-Instruct-v1.gguf` | 3.2 GB | F16 (最高品質) |

### HuggingFace

- LoRAアダプター: [`yukihamada/Photon-1.7B-Instruct-v1`](https://huggingface.co/yukihamada/Photon-1.7B-Instruct-v1)

## ライセンス

- ベースモデル: Qwen License
- 学習コード・データ: MIT License
