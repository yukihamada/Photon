# iPhoneで動く日本語AI「Photon」を100円で作ってみた

## はじめに

最近、ローカルで動くLLM（大規模言語モデル）が熱いです。ChatGPTやClaudeはすごいけど、ネット接続が必要だし、プライバシーも気になる。そこで僕は「iPhoneだけで完結する日本語AI」を作ることにしました。

**結論から言うと、学習コストはなんと約100円（$0.70）で作れました。**

データ生成にAPIを使ったので、そちらは3,000〜4,000円くらいかかりましたが、それでも笑っちゃうくらい安くできた上に、思った以上に性能が良かったので、その過程と結果を共有します。

## なぜ自分で作ったのか

iPhoneでローカルLLMを動かすアプリ（[ElioChat](https://eliochat.com)）を開発していて、いろんな日本語モデルを試してきました。

- **TinySwallow-1.5B**（Sakana AI）
- **RakutenAI-7B**（楽天）
- **Sarashina2.2-3B**（SB Intuitions）
- **Gemma-2-2B-JPN-IT**（Google）

でも、どれも微妙に納得できなかったんです。

| 問題点 | 詳細 |
|--------|------|
| 思考過程が見えない | 答えだけ出てきて「なぜ？」が分からない |
| 論理ミスがある | 「A>B>C、一番高いのは？」で「C」と答えるモデルも |
| 日本語が不自然 | 英語ベースモデルの限界を感じる |

## Photon-1.7Bの特徴

そこで作ったのが「**Photon-1.7B-Instruct-v1**」です。

### `<think>`タグで思考を可視化

一番の特徴は、**思考過程が見える**こと。

```
質問: 2の10乗はいくつですか？

Photonの回答:
<think>
1. 2の1乗：2^1 = 2
2. 2の2乗：2^2 = 2 × 2 = 4
3. 2の3乗：2^3 = 2 × 2 × 2 = 8
...
10. 2の10乗：2^10 = 1024
</think>

答えは1024です。
```

他のモデルは「1024です」と結論だけ。Photonは**なぜその答えなのか**が分かります。

### 実際の比較結果

4つの質問で比較してみました：

#### 論理テスト：「AはBより背が高い。BはCより背が高い。一番高いのは誰？」

| モデル | 回答 | 結果 |
|--------|------|------|
| **Photon-1.7B** | 「A > B > C となる。一番背が高いのは **A** です。」 | ✅ 正解 |
| TinySwallow-1.5B | 「一番背が高いのはCです。A > B > C となります。」 | ❌ 矛盾した回答 |
| RakutenAI-7B | （応答ループが発生） | ⚠️ エラー |

TinySwallowは「A > B > C」と正しく推論しておきながら、結論で「C」と言ってしまう。これが実際に起きた出来事です。

#### 日本文化：「一期一会」の意味

**Photon-1.7B**
```
<think>
「一期一会」は日本の伝統的な価値観を象徴するものです。
「一期」は「一度」、「一会」は「会う」を指します。
人生の中で一度だけの出会いを大切にする精神です。
</think>

仏教の思想に基づいた言葉で、瞬間の価値を重視する心遣いを示しています。
```

**TinySwallow-1.5B**
```
一生に一度だけの出会いを大切にすること。禅の教えから来ています。
```

どちらも正しいですが、Photonは「なぜそう言えるのか」まで説明してくれます。

## どうやって作ったのか

### 1. ベースモデルの選定

**Qwen3-1.7B**（Alibaba製）を選びました。
- 1.7Bパラメータでコンパクト
- 日本語対応
- Sakana AIのTinySwallowもこれベース

### 2. 学習データの生成

DeepSeek-R1やClaude APIを使って、1,166件の日本語データを生成しました。

| カテゴリ | 件数 |
|---------|------|
| 論理・数学・推論 | 515 |
| 日本語・文化 | 193 |
| 会話・ユーモア | 116 |
| 実用・教養 | 153 |
| その他 | 189 |

ポイントは**蒸留（Distillation）**という手法。賢いモデル（DeepSeek-R1）に「`<think>`タグを使って推論過程を書いて」と指示し、その回答を学習データにしました。

### 3. 学習

Lambda LabsのA100 GPUで学習。

| 項目 | 値 |
|------|-----|
| 学習時間 | 32.7分 |
| GPU | NVIDIA A100-SXM4-40GB |
| **コスト** | **$0.70（約100円）** |
| 最終Loss | 1.25 |

LoRA（Low-Rank Adaptation）という軽量な学習方法を使ったので、フル学習の1/10以下のコストで済みました。

### 4. GGUF変換

iPhoneで動かすために、GGUFフォーマットに変換して量子化。

| ファイル | サイズ |
|----------|--------|
| Q5_K_M.gguf | 1.2 GB |
| F16.gguf | 3.2 GB |

1.2GBなら、iPhone 13以降で快適に動きます。

## 総合評価

| モデル | パラメータ | サイズ | 思考タグ | 特徴 |
|--------|-----------|--------|----------|------|
| **Photon-1.7B** | 1.7B | 1.2GB | ✅ 100% | 思考が見える |
| TinySwallow-1.5B | 1.5B | 1.0GB | ❌ 0% | 高速だが論理ミスあり |
| RakutenAI-7B | 7B | 4.2GB | ❌ 0% | 大規模だが応答不安定 |
| Qwen3-1.7B | 1.7B | 1.7GB | ❌ 0% | ベースモデル |

Photonの強みは「**透明性**」です。AIが何を考えてその答えを出したのかが分かるので、間違っていても「ここが違う」と指摘できます。

## 使い方

### ダウンロード

**HuggingFace**: [yukihamada/Photon-1.7B-Instruct-v1](https://huggingface.co/yukihamada/Photon-1.7B-Instruct-v1)

GGUFファイルをダウンロードして、お好みのアプリで使えます：
- [ElioChat](https://eliochat.com)（iOS）
- [LM Studio](https://lmstudio.ai/)（Mac/Windows）
- llama.cpp（CLI）

### オフラインで使える

**電波がないところでも使えます。**

- 飛行機の中
- 山の中
- 無人島
- 地下鉄

プライバシーも完璧。すべてのデータがデバイス内で完結します。

## オープンソース

モデルもデータもコードも全て公開しています：

- **GitHub**: [yukihamada/Photon](https://github.com/yukihamada/Photon)
- **HuggingFace**: [yukihamada/Photon-1.7B-Instruct-v1](https://huggingface.co/yukihamada/Photon-1.7B-Instruct-v1)

GitHub Actionsでワークフローも組んでいるので、データを更新すれば自動で再学習されます。みんなでデータを追加していけば、どんどん賢いモデルになっていきます。

## 技術よりセンス

正直、この分野は**技術力よりも「どんな日本語を教えるか」**が重要だと感じました。

- どんな言い回しが自然か
- どんな思考過程が分かりやすいか
- どんな知識が役立つか

これは完全に文系的なセンスの領域で、僕があまり得意なところではありません。でも、それもLLMでカバーできています。DeepSeek-R1に「こういう質問にはこう答えて」と指示すれば、センスのある回答を大量に生成してくれます。

## これからの展望

この分野、これからどんどんいろんな企業が参入してくる面白い領域だと思います。

- Apple Intelligence
- Googleのオンデバイスモデル
- 各社のSLM（Small Language Model）

僕も引き続き注目していきます。

**よかったらPhotonを使ってみてください。フィードバックお待ちしています！**

---

## リンクまとめ

| リソース | URL |
|----------|-----|
| GitHub | https://github.com/yukihamada/Photon |
| HuggingFace | https://huggingface.co/yukihamada/Photon-1.7B-Instruct-v1 |
| ElioChat（iOSアプリ） | https://eliochat.com |

---

*この記事で使用したデータ・コード・モデルはすべてオープンソースで公開しています。MIT/Qwen Licenseで自由にお使いください。*
